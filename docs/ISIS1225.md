---
created: 2025-07-29T23:03:51
type: main
status: active
Teacher:
art:
inicio:
final:
credits: 3
NRC:
nota:
place:
---
#academy/202520/ISIS1225 #UAndes #class   
___
# [ISIS1225](https://eerosales24.github.io/eda_2025_20/#/)

## Cronograma
___
```tasks
not done
sort by priority
sort by tag
short mode
group by due
tags includes #academy/202520/ISIS1225   
```
### Ex√°menes
- [x] #academy/202520/ISIS1225 EVA1 Son solo dos puntos ‚è´ üìÖ 2025-09-04 ‚úÖ 2025-09-04
	- [ ] Verificar presentaciones
	- [ ] Tener muy claro casos espec√≠ficos de c√≥digo
	- [ ] Estudiar el bono que nunca fue
	- [ ] Estudiar estructuras de los laboratorios
	- [ ] array_list
	- [ ] single_linked_lists
- [x] #academy/202520/ISIS1225 EVA2 ‚è´ üìÖ 2025-10-09 ‚úÖ 2025-10-09
- [ ] #academy/202520/ISIS1225 EVA3 ‚è´ üìÖ 2025-11-06 
- [ ] #academy/202520/ISIS1225 EVA4 ‚è´ üìÖ 2025-12-01 
### Retos
- [x] #academy/202520/ISIS1225 Reto1 release üìÖ 2025-08-21 ‚úÖ 2025-08-20
- [x] #academy/202520/ISIS1225 Reto2 release ‚úÖ 2025-09-23
- [x] #academy/202520/ISIS1225 Reto3 release üìÖ 2025-10-16 ‚úÖ 2025-10-16
- [ ] #academy/202520/ISIS1225 Reto4 release üìÖ 2025-11-11
- [x] #academy/202520/ISIS1225 Reto1 entrega üîº üìÖ 2025-09-16 ‚úÖ 2025-09-16
- [x] #academy/202520/ISIS1225 Reto2 entrega üîº üìÖ 2025-10-17 ‚úÖ 2025-10-18
- [ ] #academy/202520/ISIS1225 Reto3 entregaüîº üìÖ 2025-11-12 
- [ ] #academy/202520/ISIS1225 Reto4 entregaüîº üìÖ 2025-12-03 
### Laboratorios

- [x] #academy/202520/ISIS1225 LAB1 release üìÖ 2025-08-07 ‚úÖ 2025-08-07
- [x] #academy/202520/ISIS1225 LAB1 entrega üìÖ 2025-08-15 ‚úÖ 2025-08-14

- [x] #academy/202520/ISIS1225 LAB2 release üìÖ 2025-08-14 ‚úÖ 2025-08-14
- [x] #academy/202520/ISIS1225 LAB2 entrega üìÖ 2025-08-20 ‚úÖ 2025-08-20

- [x] #academy/202520/ISIS1225 LAB3 release üìÖ 2025-08-21 ‚úÖ 2025-08-20
- [x] #academy/202520/ISIS1225 LAB3 entrega üìÖ 2025-08-27 ‚úÖ 2025-08-26

- [x] #academy/202520/ISIS1225 LAB4 release üìÖ 2025-08-28 ‚úÖ 2025-08-28
- [x] #academy/202520/ISIS1225 LAB4 entrega üìÖ 2025-09-03 ‚úÖ 2025-09-03

- [x] #academy/202520/ISIS1225 LAB5 parte 1 release üìÖ 2025-09-11 ‚úÖ 2025-09-11
- [x] #academy/202520/ISIS1225 LAB5 parte 1 entrega üìÖ 2025-09-17 ‚úÖ 2025-09-18

- [x] #academy/202520/ISIS1225 LAB5 parte 2 release üìÖ 2025-09-18 ‚úÖ 2025-09-18
- [x] #academy/202520/ISIS1225 LAB5 parte 2 entrega üìÖ 2025-09-24 ‚úÖ 2025-09-25

- [x] #academy/202520/ISIS1225 LAB6 release üìÖ 2025-09-26 ‚úÖ 2025-09-26
- [x] #academy/202520/ISIS1225 LAB6 entrega üìÖ 2025-10-10 ‚úÖ 2025-10-11

- [x] #academy/202520/ISIS1225 LAB7 release üìÖ 2025-10-16 ‚úÖ 2025-10-16
- [x] #academy/202520/ISIS1225 LAB7 entrega üìÖ 2025-10-22 ‚úÖ 2025-10-23

- [x] #academy/202520/ISIS1225 LAB8 release üìÖ 2025-10-23 ‚úÖ 2025-10-23
- [x] #academy/202520/ISIS1225 LAB8 entrega üìÖ 2025-10-30 ‚úÖ 2025-10-30

- [x] #academy/202520/ISIS1225 LAB9 release üìÖ 2025-10-30 ‚úÖ 2025-10-30
- [ ] #academy/202520/ISIS1225 LAB9 entrega üìÖ 2025-11-07

- [ ] #academy/202520/ISIS1225 LAB10 parte 1 release üìÖ 2025-11-13  
- [ ] #academy/202520/ISIS1225 LAB10 parte 1 entrega üìÖ 2025-11-19

- [ ] #academy/202520/ISIS1225 LAB10 parte 2 release üìÖ 2025-11-20  
- [ ] #academy/202520/ISIS1225 LAB10 parte 2 entrega üìÖ 2025-11-26

## W01M1
* La idea del curso es organizar datos
* Cumplir restricciones y crit. de calidad
	* Tiempo
	* Espacio
* Los datos se pueden organizar en
	* Listas
	* Pilas
	* Colas
* Y las estructuras son 
	* Arboles
	* Tablas (Hash)
	* Grafos
### Arquitectura de aplicaci√≥n
* Se divide en **dos**
	* La ==vista==, parte necesario para que el usuario haga uso de la aplicaci√≥n
	* La ==l√≥gica==, realiza las op. sobre los datos y NO **DEPENDE DE LA VISTA**
>[!note] La l√≥gica es as√≠ "Vista <= L√≥gica <= Modelos <= Datos"

### Uso de GIT
* Tendremos tres estados
	* modified
	* staged
	* committed
### Algoritmos y complejidad
Caracter√≠sticas:
- Precisos
- Correctos 
- Determin√≠sticos
- Finito
Cualidades:
- General
- Eficiente
Siendo as√≠ separamos el an√°lisis de algoritmos en dos categor√≠as importantes, _temporal_ y **espacial**.
## W01M2
* El algoritmo de busqueda binar√≠a es un ejemplo de un algoritmo m√°s eficiente que otro en otra tarea... ¬øPero porque?
### Tiempo de ejecuci√≠on
* No todos los algoritmos vana  ser iguales, y vamos a ver que estos tienen una complejidad que involucra el espacio en memoria que ocupan y el tiempo que tardan en cumplir su tarea.
* ### Aproximaci√≥n a prior√≠
- No depende del hardware ni de la ejecuci√≥n.
    
- Usa y considera **todos los datos**.
    
- Tres notaciones principales:
    
    - **Big Theta (Œò):** Crecimiento pr√°cticamente igual en todos los casos.
        
    - **Big Omega (Œ©):** Crecimiento m√≠nimo garantizado (mejor caso).
        
    - **Big O (O):** Crecimiento m√°ximo (peor caso).
        

---

**7 √≥rdenes de crecimiento temporal t√≠picos**

1. **O(1) ‚Äì Constante:**
    
    - Tiempo no depende del tama√±o de los datos.
        
    - Ejemplo: `len(lista)`, comparaciones, m√≥dulo.
        
    - Todas las operaciones primitivas son constantes.
        
2. **O(n) ‚Äì Lineal:**
    
    - Tiempo crece proporcionalmente con la cantidad de datos.
        
    - Ejemplo: recorrer una lista una vez.
        
    - _Big O_ suele considerar el **peor caso**.
        
3. **O(log n) ‚Äì Logar√≠tmico:**
    
    - Crecimiento lento del tiempo con respecto al tama√±o.
        
    - Aparece en divisiones sucesivas y algoritmos tipo b√∫squeda binaria.
        
4. **O(n log n) ‚Äì Lineal√≠tmico:**
    
    - Ejemplo t√≠pico: algoritmos de ordenamiento eficientes.
        
5. **O(n¬≤) ‚Äì Cuadr√°tico**
    
6. **O(n¬≥) ‚Äì C√∫bico**
    
7. **O(b‚Åø) ‚Äì Exponencial**
## W02M1
### Aproximaci√≥n (emp√≠rica a posteriori)
**Ventajas**
* Realista
* Sencilla
**Desventajas**
* Depende del entorno
	* Del hardware, S.O., programas concurrentes, etc. 
* No se puede generalizar
	* Informaci√≥n limitada a tama√±os de entrada espec√≠ficos
		* No aplica a todos los tama√±os
* Influencia de factores externos
* Carga del sistema y gesti√≥n de memoria pueden distorsionar resultados.
### Complejidad Espacial
* Mide la memoria usada por un algoritmo en relaci√≥n al tama√±o de entrada
* Generalmente basado en el peor caso de uso de memoria
* Incluye la memoria usada por
	* Variables 
	* Asignaciones din√°micas
	* ==Estructuras de datos==
#### Espacio constante O(1)
* El espacio no depende del tama√±o de la entrada
```run-python
def suma(num1, num2):
	suma = num1 + num2
	return suma
print(suma(2,2))
```
* Es idenpendiente del tamao de `num1` y `num2`
	* Solo necesita espacio constante para `suma`
#### Espacio lineal O(n)
* El espacio crece linealmente con el tama√±o de la entrada
```run-python
lista = [1,2,3]
def duplicar_lista(lista):
	lista_duplicada = []
	for elemento in lista:
		lista_duplicada.append(elemento * 2)
	return lista_duplicada
print(duplicar_lista(lista))
```
* El espacio necesario para `lista_duplicada` es igual al de la lista que estamos intentando duplicar, por lo tanto **crece linealmente**.
#### Espacio auxiliar
* Memoria adicional aparte de la entrada
```run-python
lista = [5,7,89,3,5]
def ordenar_lista(lista):
	lista_ordenada = sorted(lista)
	return lista_ordenada
print(ordenar_lista(lista))
```
* La `lista_ordenada` es una copia de la lista
	* Resultado en un espacio adicional
		* Proporcional al tama√±o de lista original

>[!question] No entiendo la diferencia entre espacio lineal O(n) y el espacio auxiliar

Diferencia entre **O(n) espacio** y **espacio auxiliar O(n)**
* ***Espacio total**: Toda la memoria que usa el algoritmo (entrada + salida + variables temporales).
* ***Espacio auxiliar**: Memoria **extra** necesaria aparte de la entrada.
* 
| Caso | Entrada (`n`) | Memoria adicional | Espacio total | Espacio auxiliar |
|------|---------------|------------------|---------------|------------------|
| Duplicar lista | `n` | `n` | `O(n)` | `O(n)` |
| Ordenar lista (copia) | `n` | `n` | `O(n)` | `O(n)` |
| Ordenar in-place | `n` | constante | `O(n)` | `O(1)` |

Conclusi√≥n: 
- **O(n) espacio**: Memoria total crece proporcionalmente a `n`.
- **Espacio auxiliar O(n)**: Memoria extra (aparte de la entrada) crece proporcionalmente a `n`.
- Un algoritmo puede tener **espacio total O(n)** pero **auxiliar O(1)** si modifica la entrada directamente.

### Recursividad
* La funci√≥n se invoca a s√≠ misma
	* Resuelve instancia m√°s peque√±a que el problema general
* En ejemplo cl√°sico son las mu√±ecas rusas
	* Versiones m√°s peque√±as anidadas
	* Profundidad **finita**
* Como en [[MATE1102]] hay dos casos
	* Caso base
		* Condici√≥n que detiene la recursi√≥n
		* La funci√≥n no se invoca a si misma
	* Caso recursivo
		* Funci√≥n que se invoca a s√≠ misma
		* Es una versi√≥n ==m√°s peque√±a== del problema
### Metodolog√≠a de desarollo de sofware
Aqu√≠ el orden importa
>[!warning] An√°lisis > Dise√±o > Construcci√≥n > Pruebas
#### An√°lisis
1. Identificar y especificar el problema
2. Identificar restrucciones
3. Documentar
* La idea es planear desde antes que es lo que se va a realizar y que tiene que hacer la funci√≥n que vamos a realizar
* Hay que entender su **proposito** y adem√°s que entradas y salidas vamos a esperar/dise√±ar
* Finalmente tambi√©n es importante pensar en las restricciones del mismo y como estas van a afectar el dise√±o. Estas aplican tanto para ==entrada como para salida==

* Una vez hemos hecho e identificado el profeso arriba ya podemos escribir la documentaci√≥n:

```run-python

"""
Calcula el factorial de un n√∫mero (n!)

param numero: El n√∫mero positivo para calcular su factorial
:type numero: int

:returns: El factorial del n√∫mero proporcionado
:rtype: int
"""
# Nota este de arriba es el formato de documentaci√≥n para EDA.

```

Note que la documentaci√≥n refleja claramente las restricciones y la naturaleza de la entrada/salida.



#### Dise√±o
1. Formular ejemplo
	1. Casos significativos y no redundantes
2. Dise√±ar algoritmo
3. Dise√±ar y documentar las pruebas

`
factorial (0) = 1
factorial (1) = 1
factorial (2) = 2
.... etc.
`
## Repaso Bono W02M1
```run-python
import matplotlib.pyplot as plt
import numpy as np

# Valores de n
n_values = np.arange(1, 20)

# Crecimientos
linear = n_values
quadratic = n_values**2
nlogn = n_values * np.log2(n_values)
exponential = 2**n_values

plt.figure(figsize=(8,5))
plt.plot(n_values, linear, label=r"$n$", marker='o')
plt.plot(n_values, quadratic, label=r"$n^2$", marker='o')
plt.plot(n_values, nlogn, label=r"$n \log n$", marker='o')
plt.plot(n_values, exponential, label=r"$2^n$", marker='o')

plt.ylim(0, 200)  # limitar para ver las curvas m√°s peque√±as
plt.xlabel("Tama√±o de entrada n")
plt.ylabel("N√∫mero de operaciones (escala lineal)")
plt.title("Comparaci√≥n de √≥rdenes de crecimiento")
plt.legend()
plt.grid(True)
plt.show()

```
Aqu√≠ ves c√≥mo crecen las funciones:

- **O(n)** crece de forma lineal.
    
- **O(n log n)** crece un poco m√°s r√°pido, pero sigue siendo mucho menor que cuadr√°tica para valores grandes.
    
- **O(n¬≤)** crece muy r√°pido.
    
- **O(2‚Åø)** explota enseguida, incluso con n peque√±o.
    

Esto es por lo que un algoritmo **exponencial** se vuelve impracticable muy r√°pido.
### Complejidad temporal
```run-python
lista = [7,6,4,3,1,5,9]
elemento_a_buscar = 5
def busqueda_secuencial(lista, elemento_a_buscar):
	resultado = None
	encontrado = False
	indice = 0 
	while indice < len(lista) and not encontrado:
		if lista[indice] == elemento_a_buscar:
			resultado = lista[indice]  
			encontrado = True
		indice += 1 

	return resultado
print(busqueda_secuencial(lista, elemento_a_buscar))
```
y ahora comparemoslo al algoritmo de busqueda binar√≠a
```run-python
lista = [1,2,3,4,5,6]
elemento_a_buscar = 5
def busqueda_binaria(lista, elemento_a_buscar):
	inicio, fin = 0, len(lista) - 1
	resultado = None
	encontrado = False
	while inicio <= fin and not encontrado:
		medio = (inicio + fin)//2
		if lista[medio] < elemento_a_buscar:
			inicio = medio  + 1
		elif lista[medio] > elemento_a_buscar:
			inicio = medio  - 1
		else:
			resultado = lista[medio]
			encontrado = True
	return resultado
print(busqueda_binaria(lista, elemento_a_buscar))

```
* Uno es mucho m√°s r√°pido que el otro, pero porque exactamente....
* Partimos del m√©todo cient√≠fico


>[!note] Complejidad temporal
> Tiempo total de ejecuci√≥n = N√∫mero de operaciones requeridas * Tiempo por operaci√≥n

>[!note] Complejidad espacial
> Memoria total = N√∫mero objetos requeridos * Memoria por objeto

#### Aproximaci√≥n te√≥rica (a prori)
* Determinaci√≥n aproximaci√≥n matem√°tica
* Ventajas: No requiere la implementaci√≥n del algoritmo
* No depende del hardware o sofware de soporte
	* Independiente del tama√±o de los datos de entrada
	* Considera todos los datos de entrada
#### Notaciones para la complejidad algoritmica
1. Big Theta $\Theta$
	1. Que es la que mide exactamente como se comporta el algoritmo en terminos de tiempo vs tama√±o de datos (L√≠mite asintotico de los datos)
	2. Poco usada porque es muy dificil de obtener
	3. Baja aplicabilidad
2. Big Omega $\Omega$
	1. Es b√°sicamente el l√≠mite inferior asint√≥tico 
3. Big O $O$
	1. Establece el l√≠mite superior asint√≥tico
	2. Se usa para describir la complejidad m√°xima de un algoritmo, y siempre va por arriba del estimado en la gr√°fica de tiempo vs tama√±o de los datos.
	3. Siempre estima por arriba. Es MUY INFORMATIVO.
#### Orden constante $O(1)$
	1.  Tiempo constante y predecible
	2. No aumenta con el tama√±o de la entrada, osea indpendiente del tama√±o de los datos
Ejemplos de operaciones primitivas o constantes:
![[Pasted image 20250811200137.png]]

Ahora un ejemplo de algoritmo de orden constante

```run-python
def es_par(n):
    """
    Determina si un n√∫mero es par o impar.
    Args:
        n (int): el n√∫mero a verificar.
    Returns:
        bool: True si n es par, False si n es impar.
    """
    return n % 2 == 0
print(es_par(5))
```

* Note que realmente solo se esta usando funciones que tienen ordenes constantes $O(1)$

Quiz1
```run-python
lista = [1,2,3,4,5,6]
pos = 0
def dar_elemento_en_posicion(lista, pos):
    """
    Retorna el elemento en la posici√≥n especificada de la lista.

    Args:
        lista (list): La lista de la cual obtener el elemento.
        pos (int): La posici√≥n del elemento a retornar.

    Returns:
        El elemento en la posici√≥n dada o None si la posici√≥n es inv√°lida.
    """
    resultado = None
    if 0 <= pos < len(lista):
        resultado = lista[pos]
    return resultado
print(dar_elemento_en_posicion(lista, pos))
```
Este como vemos tiene una complejidad temporal de $O(1)$ porque solo se utilizan operaciones primitivas que se repiten una unica vez. No hay ciclos y no, if no es un ciclo. Todas son comparaciones. Y recuerde ==PRINT() NO ES $O(1)$==.

#### Orden lineal $O(n)$
- Caracter√≠sticas
    - Tiempo crece proporcionalmente con la cantidad de datos
    - T√≠picamente, cada elemento se procesa una vez
- Ventajas
    - Simple de entender e interpretar
    - Eficiente para vol√∫menes de datos peque√±os o medianos
- Desventajas
    - Ineficiente a gran escala
un ejemplo es:

```run-python
for elemento in lista:
    # Expresiones O(1)
```
Es f√°cil ver como el tiempo requerido para correrlo va a crecer de manera lineal con la cantidad de datos.
```run-python
def sumar_elementos(lista):
    '''
    Suma todos los elementos de una lista.

    Args:
        arreglo (list): Una lista de n√∫meros (int o float).

    Returns:
        int o float: La suma de todos los elementos en la lista.
    '''
    total = 0
    for elemento in lista:
        total += elemento
    return total
```
De nuevo es lineal porque note que las operaciones $O(1)$ se estan repitiendo n veces, las n veces corresponden a los n elementos de la lista.

```run-python
def sumar_elementos(lista):
    total = 0
    for elemento in lista:
        total += elemento
    return total
```

Otro ejemplo quee es lineal pues va a depender de que tantos elementos busquemos sumar, esta es una buena forma de verificar si el comportamiento que vemos es $O(1)$ o $O(n)$.

Y acon que haya una sola operaci√≥n que se repita `n` veces es suficiente para que sea un orden lineal.
}

#### An√°lisis de algoritmos por casos
- El an√°lisis de algoritmos no constantes, t√≠picamente considera tres casos:
- **Peor caso:**
    - Input que requiere el m√°ximo tiempo de ejecuci√≥n del algoritmo
- **Mejor caso:**
    - Input que requiere el m√≠nimo tiempo de ejecuci√≥n del algoritmo
- **Caso promedio:**
    - Estimaci√≥n promedio del tiempo de ejecuci√≥n
        - Considerando la distribuci√≥n probabil√≠stica del input
Por ejemplo:
```run-python
for elemento in lista:
    if elemento == valor_buscado:
        # Valor encontrado
        # Fin de b√∫squeda
```
En este caso, si el primer valor de la lista es el valor que buscamos vamos a obtener el mejor de los casos, osea un $O(1)$, pero lo m√°s probable es que esto no sea verdad y que tengamos que repetir la igualdad del if por cada elemento (hay un for entonces repetimos la instrucci√≥n), esto quiere decir que vamos a terminar con un $O(n)$ en promedio y el peor de los casos no vamos a encontrar el dato pero si estamos revisando cada cosa entonces va a ser $O(n)$ tambi√©n.
>[!tip] Recuerde que la notaci√≥n Bif $O$ usa siempre el peor caso

Por ejemplo en el caso de la busqueda_secuencial encontramos que en efecto se trata de que no encontraremos el dato, y en notaci√≥n $O$, esto quiere decir que va a tener una complejidad $O(n)$.

#### Orden logar√≠tmico O(log n)
- Caracter√≠sticas
    - Tiempo incrementa lentamente a medida que crecen los datos
- Ventajas
    - Eficiente para grandes cantidades de datos
- Desventajas
    - Mayor complejidad de implementaci√≥n
    - Costos de ordenamiento y mantenimiento de datos
___
- Caracter√≠stica clave:
    - Reducci√≥n significativa y r√°pida del rango de operaci√≥n
- Explicaci√≥n:
    - Variable se `i` divide por `c` en cada iteraci√≥n
        - Disminuci√≥n hasta que `i <= 1`
- Iteraciones limitadas a logc de `n`
    - En complejidad temporal, no importa el coeficiente constante de log
        - Entonces, la complejidad logar√≠tmica es **O(log n)**
Ejemplo:
```run-python
i = n
while i > 0:
    # Expresiones O(1)
    i /= c  # √≥ divisi√≥n entera
```
```run-python
i = 1
while i < n:
    # Expresiones O(1)
    i *= c
```
Son b√°sicamente expresiones O(1), pero note que hay una divisi√≥n o una exponencial que me va cortando las posibilidades hasta que el algoritmo termina de correr, as√≠ b√°sicamente aplicando la estrategia de divide y venceras.

Recordemos entonces que la ==b√∫squeda binar√≠a== va a tener este tipo e complejidad temporal.

>[!tldr] Caracter√≠stica clave
> Itera reduciendo la b√∫squeda a la mitad en cada paso

#### B√∫squeda secuencial vs b√∫squeda binaria (2/2)
- B√∫squeda secuencial:
    
    - **Orden lineal O(n):**
        - Simple pero ineficiente para grandes cantidades de datos
- B√∫squeda binaria:
    
    - **Orden logar√≠tmico O(log n):**
        - M√°s complejo y requiere ordenamiento, pero,
            - Muy eficiente para grandes cantidades de datos
#### Orden linear√≠tmico O(n log(n))
- Caracter√≠sticas
    - Combinaci√≥n de lineal y logar√≠tmico:
        - Tiempo crece proporcional a n y a n log n
    - M√°s r√°pido que lineal para grandes n, pero m√°s lento que logar√≠tmico
- Ventajas
    - Eficiente de moderadas a grandes vol√∫menes de datos
- Desventajas
    - Mayor complejidad de implementaci√≥n
    - M√°s lento que algoritmos logar√≠tmicos puros
    - Puede ser muy costoso para algunos vol√∫menes de datos grandes
Ejemplo:
```run-python
for i in range(1, n):  # Parte lineal
    # Expresiones O(1)
    j = 1
    while j < n:  # Parte logar√≠tmica
        # Expresiones O(1)
        j *= 2
```
Otro ejemplo un poco m√°s elaborado:
```run-python
def buscar_en_sublistas_con_bbinaria(listas, elemento_buscado):
    """
    Busca un elemento en cada sublista ordenada de la lista principal.

    Args:
        listas (list of list): Lista de sublistas ordenadas.
        elemento_buscado: el elemento a buscar.

    Returns:
        tuple: (√≠ndice_lista_principal, √≠ndice_sublista) del elemento
        encontrado, o (None, None) si el elemento no se encuentra.
    """
    resultado = (None, None)
    encontrado = False

    for indice_lista_principal, sublista in enumerate(listas):
        if not encontrado:
            indice_sublista = busqueda_binaria(sublista, elemento_buscado)
            if indice_sublista is not None:
                resultado = (indice_lista_principal, indice_sublista)
                encontrado = True
    return resultado
```
Note que b√°sicamente se esta haciendo una busqueda lineal, la cual ya sabemos que tiene una complejidad temporal de O(n), y dentro de esta se esta buscando las sublistas mediante el m√©todo de busqueda binaria, que es O(log n), lo cual resulta en una complejidad espacial O(n log m)

#### Orden cuadr√°tico O(n2)
- Caracter√≠sticas
    
    - El tiempo de ejecuci√≥n escala con el cuadrado del tama√±o de la entrada
        
    - Tiempo proporcional a n¬≤
        
    - Efectivo para peque√±as entradas,
        
        - Pero impr√°ctico r√°pidamente con grandes vol√∫menes
- Ventajas
    
    - Simple de entender e interpretar
        
    - Eficiente para conjuntos de datos muy peque√±os, con operaciones espec√≠ficas
        
- Desventajas
    
    - Ineficiente para grandes vol√∫menes de datos
Ejemplo:
```run-python
for i in range(n):
    for j in range(n):
        # Expresiones O(1)
```
El hecho de tener dos loops anidados, o dos cosas que dependen de n autom√°ticamente lo hace $O(n^2)$

#### Complejidad temporal - Shaker Sort (1/3)

Estime el orden de crecimiento temporal del siguiente algoritmo:

```run-python
def cocktail_shaker_sort(arr: list[int]) -> None:
    start = 0
    end = len(arr) - 1
    swapped = True

    while swapped:
        swapped = False
        # Movimiento de izquierda a derecha
        for i in range(start, end):
            if arr[i] > arr[i + 1]:
                arr[i], arr[i + 1] = arr[i + 1], arr[i]
                swapped = True
        end -= 1

        # Movimiento de derecha a izquierda
        for i in range(end, start, -1):
            if arr[i] < arr[i - 1]:
                arr[i], arr[i - 1] = arr[i - 1], arr[i]
                swapped = True
        start += 1
```
>[!warning] Ojo el Shaker sort SI ES $O(n^2)$
#### Orden c√∫bico O(n¬≥)

- Caracter√≠sticas
    
    - Tiempo de ejecuci√≥n proporcional al cubo del tama√±o de la entrada
    - Efectivo para peque√±as entradas
        - Pero impr√°ctico r√°pidamente con grandes vol√∫menes
- Ventajas
    
    - Puede ser adecuado para problemas
        - Que requieren operaciones tridimensionales
- Desventajas
    
    - Muy ineficiente a medida que el tama√±o de la entrada crece
```run-python
for i in range(n):
    for j in range(n):
        for k in range(n):
            # Expresiones O(1)
```
```run-python
def buscar_tripletes(lista):
    """
    Encuentra todos los tripletes que sumen cero.

    Args:
        lista (list): lista de n√∫meros enteros.

    Returns:
        list of tuples: lista de tripletes (i, j, k), donde i + j + k == 0.
    """
    triplete = []
    for i in lista: # Se ejecuta n veces
        for j in lista: # Se ejecuta n veces, por cada iteraci√≥n de i
            for k in lista: # Se ejecuta n veces, por cada iteraci√≥n de j
                if i + j + k == 0: # Se ejecuta n * n * n
                    triplete.append((i, j, k)) # Se ejecuta hasta n * n * n
    return triplete
```
#### Orden exponencial O(b‚Åø)

- Caracter√≠sticas
    
    - Crecimiento muy r√°pido
        
        - El tiempo aumenta exponencialmente con el tama√±o de la entrada
        - Tiempo proporcional a `b‚Åø`, donde `b` y `n` son `> 1`
    - T√≠picamente involucra algoritmos que exploran todas las posibilidades
        
- Ventajas
    
    - Apropiado para problemas peque√±os donde se requieren soluciones exhaustivas
- Desventajas
    
    - Generalmente no es viable para grandes vol√∫menes de datos
```run-python
def generar_combinaciones(conjunto):
    """
    Genera todas las combinaciones posibles de un conjunto.
    
    Args:
        conjunto: Lista que representa el conjunto (Ej.:, ["A", "B", "C"])
        
    Retrurns:
        list of lists: Todas las combinaciones posibles
    """
    n = len(conjunto)
    total_combinaciones = 2 ** n  # N√∫mero total de combinaciones
    combinaciones = []

    for i in range(total_combinaciones):
        combinacion_actual = []
        for j in range(n):
            # Verifica si el bit j est√° encendido en el n√∫mero i:
            if i & (1 << j):   
                combinacion_actual.append(conjunto[j])
        combinaciones.append(combinacion_actual)
    return combinaciones
```

#### Aproximaci√≥n emp√≠rica (_a posteriori_ ) (3/3)

```run-python
import time  
  
start_time = time.time()  
  
# Your code or operation to be timed goes here  
# Example:  
sum(range(10**2))  
  
end_time = time.time()  
elapsed_time = end_time - start_time  
  
print(f"Elapsed time: {elapsed_time:.4f} seconds")
```
- "Realismo"¬†
    
    - Mide comportamiento directo¬†
        - Considerando constantes y algunos factores secundarios
- "Sencillez"¬†
    
    - Puede¬† ser m√°s f√°cil de implementar que algunos an√°lisis te√≥ricos
        - Especialmente en algoritmos complejos
### Complejidad Espacial
* Mide la memoria usada por un algoritmo en relaci√≥n al tama√±o de entrada
* Generalmente basado en el peor caso de uso de memoria
* Incluye la memoria usada por
	* Variables 
	* Asignaciones din√°micas
	* ==Estructuras de datos==
#### Espacio constante O(1)
* El espacio no depende del tama√±o de la entrada
```run-python
def suma(num1, num2):
	suma = num1 + num2
	return suma
print(suma(2,2))
```
* Es idenpendiente del tamao de `num1` y `num2`
	* Solo necesita espacio constante para `suma`
#### Espacio lineal O(n)
* El espacio crece linealmente con el tama√±o de la entrada
```run-python
lista = [1,2,3]
def duplicar_lista(lista):
	lista_duplicada = []
	for elemento in lista:
		lista_duplicada.append(elemento * 2)
	return lista_duplicada
print(duplicar_lista(lista))
```
* El espacio necesario para `lista_duplicada` es igual al de la lista que estamos intentando duplicar, por lo tanto **crece linealmente**.
#### Espacio auxiliar
* Memoria adicional aparte de la entrada
```run-python
lista = [5,7,89,3,5]
def ordenar_lista(lista):
	lista_ordenada = sorted(lista)
	return lista_ordenada
print(ordenar_lista(lista))
```
* La `lista_ordenada` es una copia de la lista
	* Resultado en un espacio adicional
		* Proporcional al tama√±o de lista original

>[!question] No entiendo la diferencia entre espacio lineal O(n) y el espacio auxiliar

Diferencia entre **O(n) espacio** y **espacio auxiliar O(n)**
* ***Espacio total**: Toda la memoria que usa el algoritmo (entrada + salida + variables temporales).
* ***Espacio auxiliar**: Memoria **extra** necesaria aparte de la entrada.
* 
| Caso | Entrada (`n`) | Memoria adicional | Espacio total | Espacio auxiliar |
|------|---------------|------------------|---------------|------------------|
| Duplicar lista | `n` | `n` | `O(n)` | `O(n)` |
| Ordenar lista (copia) | `n` | `n` | `O(n)` | `O(n)` |
| Ordenar in-place | `n` | constante | `O(n)` | `O(1)` |

Conclusi√≥n: 
- **O(n) espacio**: Memoria total crece proporcionalmente a `n`.
- **Espacio auxiliar O(n)**: Memoria extra (aparte de la entrada) crece proporcionalmente a `n`.
- Un algoritmo puede tener **espacio total O(n)** pero **auxiliar O(1)** si modifica la entrada directamente.

### Recursividad
- La funci√≥n se invoca a s√≠ misma
    - Para resolver una instancia m√°s peque√±a del problema
- Divide y vencer√°s
    - Descomponer problemas en sub-problemas
- Ejemplo en la vida¬† real ‚Üí Mu√±ecas¬† rusas
    - Versiones m√°s peque√±as anidadas
    - Profundidad finita
#### Componentes importantes
Componentes:
- **Caso base**:
    - Condici√≥n que detiene la recursi√≥n
        - Funci√≥n **no** se invoca a s√≠ misma
- **Caso recursivo**:
    - Funci√≥n se invoca a s√≠ misma
        - En una **versi√≥n m√°s peque√±a** del problema
#### Metodolog√≠a de Desarrollo de software aplicado a recursividad
>[!note] An√°lisis > Dise√±o > Construcci√≥n > Pruebas
1. **An√°lisis**
	1. Identificar y especificar el problema
	2. Identificar restricciones
		1. Se entiende que se debe definir las entradas y salidas de nuestra funci√≥n y sus respectivas restricciones.
	3. Documentar
		1. Note que la documentaci√≥n refleja las **restricciones**
			- `(int)` indica
			    - Que el tipo de dato de entrada esperado es un entero
			    - Que el tipo de dato a retornar es un entero
			- Se especifica la naturaleza positiva de la entrada
		    - El factorial es por definici√≥n positivo
Un ejemplo
```run-python
"""
Calcula el factorial de un n√∫mero (n!).

:param numero: El n√∫mero positivo para calcular su factorial.
:type numero: int

:returns: El factorial del n√∫mero proporcionado.
:rtype: int
"""
```
2. **Dise√±o**
	1. Formular ejemplos
	    - Casos significativos y no redundantes
	2. Dise√±ar el algoritmo
		1. Si el algoritmo es recursivo note que entonces deber√° especificar **caso base** y **caso recursivo**
	3. Dise√±ar y documentar las pruebas
ejemplo se ve as√≠
```run-python
"""
Calcula el factorial de un n√∫mero (n!).

:param numero: El n√∫mero positivo para calcular su factorial.
:type numero: int

:returns: El factorial del n√∫mero proporcionado.
:rtype: int

>>> factorial(0)  # Caso base
1
>>> factorial(1)  # Caso base
1
>>> factorial(2)  # Caso recursivo con 2
2
>>> factorial(3)  # Caso recursivo con 3
6
>>> factorial(4)  # Caso recursivo con 4
24
>>> factorial(5)  # Caso recursivo con 5
120
"""	"""
Calcula el factorial de un n√∫mero (n!).

:param numero: El n√∫mero positivo para calcular su factorial.
:type numero: int

:returns: El factorial del n√∫mero proporcionado.
:rtype: int

>>> factorial(0)  # Caso base
1
>>> factorial(1)  # Caso base
1
>>> factorial(2)  # Caso recursivo con 2
2
>>> factorial(3)  # Caso recursivo con 3
6
>>> factorial(4)  # Caso recursivo con 4
24
>>> factorial(5)  # Caso recursivo con 5
120
"""
```

3. Construcci√≥n 
	1. Codifica/implementa el dise√±o usando
		1. Documentaci√≥n
		2. Convenciones
		3. Buenas pr√°cticas de programaci√≥n
```run-python
def factorial(n):
# Ahora vamos a definir los casos base primero
	if n == 0 or n == 1:
		return 1
# Luego la parte recursiva
	else:
		return n * factorial(n - 1)
```
Sin embargo el c√≥digo siempre se puede pulir m√°s
```run-python
def factorial(n):
    return 1 if n <= 1 else n * factorial(n - 1)
    # Esto aprovechando algo que se conoce como Operador terniario. que hace que sea muy corta la implementaci√≥n. Evalua una expresi√≥n y de concidir una condici√≥n hace algo, de lo contrario hace otra cosa (hace o expresi√≥n1 o expresi√≥n2)
```

Un ejemplo de operador terniario

```run-python
resultado = 'Par' if num % 2 == 0 else 'Impar'
```

Note que primero se eval√∫a la condici√≥n `num % 2 == 0`, luego si esta vale se eval√∫a la expresi√≥n a la izquierda de evaluar `True` y de lo contrar√≠o la que esta a la izquierda.

A diferencia del if, **este es una expresi√≥n**

y puede ser utilizado en asignaciones

4. Finalmente debemos realizar las **pruebas** sobre la implementaci√≥n
	1. Verificar si el resultado obtenido es el esperado
	2. Depurar
		1. Corregir erroes
		2. Pulir/mejorar/optimizar
>[!bug] Aqu√≠ es super importante implementar y ejecutar los ==doctests==

```run-python
import doctest
 
def factorial(n):
    """
    Calcula el factorial de un n√∫mero.
 
    :param n: El n√∫mero positivo para calcular su factorial.
    :type n: int
 
    :returns: El factorial del n√∫mero proporcionado.
    :rtype: int
 
>>> factorial(0)  # Caso base
1
>>> factorial(1)  # Caso base
1
>>> factorial(2)
2
>>> factorial(3)
6
>>> factorial(4)
24
>>> factorial(5)
120
    """
    return 1 if n <= 1 else n * factorial(n - 1)
 
doctest.run_docstring_examples(factorial, globals(), verbose=True)
```
#### Reglas de recursividad
1. Definir el(los) caso(s) base con soluci√≥n conocida
2. Establecer el(los) caso(s) recursivo(s) que resuelvan los sub-problemas
3. Garantizar que los casos recursivos converjan al caso base
4. Si hay m√∫ltiples casos recursivos, deben ser disyuntos
5. La soluci√≥n debe combinar los resultados de los casos base y recursivos

QuizI Implementar la funci√≥n de fibonacci() recursivamente

```
fibonacci(0) = 0
fibonacci(1) = 1
fibonacci(2) = fibonacci(1) + fibonacci(0) = 1 + 0 = 1
fibonacci(3) = fibonacci(2) + fibonacci(1) = 1 + 1 = 2
fibonacci(4) = fibonacci(3) + fibonacci(2) = 2 + 1 = 3
fibonacci(5) = fibonacci(4) + fibonacci(3) = 3 + 2 = 5
‚Ä¶
fibonacci(n) = fibonacci(n‚àí1) + fibonacci(n‚àí2)
```

De nuevo vemos que hay dos casos bases, y podemos usar una estructura similar a la de el factorial

```run-python
def fibonacci(number):
	if number == 0:
		return 0
	elif number == 1:
		return 1
	else:
		return fibonacci(number-1)+fibonacci(number-2)
print(fibonacci(4))
```

ahora esa fue la implementaci√≥n fea, se puede hacer mucho mejor as√≠ 

```run-python
def fibonacci(n):
	return n if n<2 else fibonacci(n-1)+fibonacci(n-2)
print(fibonacci(24))
```


finalmente hay que incluir los docstring

La funci√≥n recursiva de Fibonacci tiene complejidad temporal **O(2^n)**, porque cada llamada genera dos llamadas adicionales, formando un √°rbol de tama√±o exponencial.  
La complejidad espacial es **O(n)**, ya que la pila de llamadas crece hasta una profundidad m√°xima de `n` antes de alcanzar el caso base.

#### Como analizar la complejidad de algoritmos recursivos
- Contabilizar cada operaci√≥n realizada en cada activaci√≥n
    - En cada invocaci√≥n de la funci√≥n
        - Considerar s√≥lo las operaciones dentro de esa activaci√≥n
- Sumar el n√∫mero de operaciones ejecutadas en todas las activaciones
    - Para obtener el total del algoritmo recursivo
En el caso del factorial vemos que tiene una complejidad de $O(n)$ porque cada activaci√≥n va a ir disminuyendo la cuenta final de a `n-1`, lo cual hace que este sea del orden `n` al finalizar.
#### Como analizar la complejidad espacial
##### Pila Stack 
- Estructura de datos con comportamiento LIFO
    - √öltimo en entrar, primero en salir (Last In, First Out)

![[Pasted image 20250811223652.png|center]]

- El Call Stack de Python es una pila
    - Almacena invocaciones a funciones y variables locales


![[Pasted image 20250811223849.png]]


>[!warning] `RecursionError`
>- Excepci√≥n que ocurre cuando:
>	- Una funci√≥n se llama a s√≠ misma demasiadas veces
>- Causa:
>	- Recursi√≥n sin caso base
>		- Solo hay caso recursivo
>	- Funci√≥n mal dise√±ada
>	- La ejecuci√≥n no alcanza el caso base
>- Python limita la profundidad recursiva (**~1000 llamadas**)
>- Ayuda: [RecursionError](https://docs.python.org/3/library/exceptions.html#RecursionError)

##### Solucionar un `RecursionError` tenga en cuenta

- Asegurar que la funci√≥n:
    
    1. Tenga **un caso base**
        
    2. **Avance hacia el o los casos base**
        
        - En cada recursi√≥n
#### Clasificaci√≥n de funciones recursivas (1/4)

- Seg√∫n la ubicaci√≥n de llamada recursiva
	- ==Recursividad directa==
	- ==Recursividad indirecta==
- Seg√∫n el n√∫mero de llamadas recursivas generadas en tiempo de ejecuci√≥n
	- Lineal o simple
		- Ej. factorial
		- Pueden ser pasadas a iterativa
	- No lineal o m√∫ltiple
		- Se generan dos o m√°s llamadas internas
		- Ej. Funci√≥n Fibonacci recusiva
- Seg√∫n la naturaleza de la llamada
    - No hay operaciones despu√©s del llamado a la recursi√≥n √≥
    - Hay operaciones pendientes despu√©s de la recursi√≥n
### Repetici√≥n en Python
#### Iterativa - ciclos
- Utiliza ciclos (como `for` o `while`)
    
    - Para repetir acciones
- Avanza mediante incrementos o decrementos expl√≠citos de variables
    
- Mantiene el estado del c√°lculo en variables temporales
    
- Mantiene un espacio de pila **constante** durante su ejecuci√≥n
* For
```run-python
def factorial(n):
    if n == 0:  # Caso especial:
        return 1
    else:       # Caso general:
        fact = 1
        for i in range(2, n + 1):
            fact *= i
    return fact
```
* While
#### Recursividad
##### Recursivo vs. Iterativo (complejidad espacial)

- **Recursivo**:
    
    - Uso de espacio de pila de llamadas proporcional a `n`
    - Puede llevar a un desbordamiento de la pila para valores muy grandes de `n`
        - Debido a la profundidad de la recursi√≥n
- **Iterativo**:
    
    - Uso de espacio de pila de llamadas constante
    - M√°s eficiente en t√©rminos de memoria para valores grandes de `n`
##### Colas en recursividad 
- **Recursividad final (Tail) - (Iterativa)**:
    
    - La llamada recursiva se realiza como √∫ltima instrucci√≥n dentro de la funci√≥n
        - No hay operaciones pendientes despu√©s de la llamada recursiva
    - Puede optimizarse para volverse recursiva
        - Se puede transformar en un ciclo internamente
    - A este tipo de recursi√≥n se le suele llamar iterativa
- **Recursividad no final (No Tail)**:
    
    - Hay operaciones pendientes despu√©s de la llamada recursiva
## W02M2 aftermath
```run-python

def contar_digitos(n):
    return 1 if n < 10 else contar_digitos(n // 10) + 1

print(contar_digitos(10))  # 2


```
## W02M3 
* ¬øQu√© es un repositorio?
* Es donde se guarda todo el c√≥digo fuente o todos los asets del proyecto.
## W03M2
### Estructuras de datos lineales I
* Secuencia de elementos ordenados 
* Con operaciones de acceso, inserci√≥n y eliminaci√≥n
>[!example] Arreglo y lista enlazada simple (Single linked list)

* Un ejemplo podr√≠a ser la lista de python
	* Secuencia mutable
		* Iterable e indexado
			* Desde 0
	* Retorna su tama√±o
	* Strings, listas y tuplas son todas secuencias
		* Caracteres, otros tipos, solo que una es mutable y la otra no
	* Soportan modificaciones despues de su creaci√≥n
	* Permite **duplicado**
	* ```
	  python[-2,-1,-0] # Es una lista de enteros
	  ```
	* Ahora si, bit
### Bit
* Unidad b√°sica de informaci√≥n
	* Representa un 1 o un 0
	* Luego esta el **Byte** (Que son 8 bits)
		* Agrupaci√≥n
		* Y cada byte tiene una direcci√≥n de memoria √∫nica
### Arreglo
* Estructura de datos
* Almacena elementos secuenciales en memoria contigua
* Por ejemplo el string en python.
* A bajo nivel un string se almacena en el hib que tiene una segmentaci√≥n.
	* Note que cada caracter ocupa **DOS SEGMENTOS**
	* Porque el character unicode necesita estos espacios.
	* Eso quiere decir que por ejemplo el string ```SAMPLE``` ocupa 12 bytes
	* Y la memoria se guarda de forma ==CONTIGUA==
* Ahora los indices que nosotros manejamos no son las direcciones de memoria
	* En este caso los indices son la sumatoria de dos segmentos.
	* Como todas ocupan la misma cantidad de celdas es que podemos acceder a __tiempo constante a cualquier indice__
		* ¬°Esto es una maravilla!
	* >[!example] Para acceder a una celda solo tenemos que hacer ```inicio + tama√±o_celda * indice*``` 
	>Que en este caso es ```2146+2*4```, en el caso en el que el arreglo empieza en el 2146, por el indice por 2. ¬°Note que es por esto que empiezan en el 0!
	> Es por esto que esta operaci√≥n es $O(1)$
	* Todo acceso es $O[1]$, adem√°s el tama√±o de acceso nunca es mayor a 2.
* Python garantiza que todas las celdas sean del mismo tama√±o
	* Si almaceno una lista de strings, entonces ya no se con antelaci√≥n como reservar un espacio fijo para que **sea eficiente**.

#### Arreglo compacto
* Almacena ed forma contigua = compacta
* Por ejemplo los strings,  porque van uno despu√©s del otro
#### Arreglo referencial
* Se guarda en referencias a objetos
* Una lista es un puntero que me manda a cada uno de los objetos que me interesan, algo como si tuvi√©ramos una lista de cosas en obsidian que me mandan a otras p√°ginas.
* ¬ø√ìsea se guarda el texto? **NO**
	* Se guardan referencias
* Por esto podemos tener listas de lo que queramos, porque solo tienen que apuntar.
* Cada una de las referencias toma un espacio √∫nico, de hecho podr√≠an estar en un solo segmento.
* Estas tienen el beneficio de tener accesos que todav√≠a son $O(1)$
##### Referencias inmutables
**Una lista no es m√°s que una referencia apuntando a objetos**
1. Tipo de dato
2. Valor 
3. Identidad
* Note que entonces cuando copiamos las listas, no se tienen que copiar las referencias 
```run-python
original = [1,2]
copy_1 = original[:] # copy_1 = [1,2]
copy_2 = original.copy() # copy_2 = [1,2]
copy_3 = copy.copy(original) # copy_3 = [1,2]
```
* Ahora los enteros son inmutables, entonces el caso donde cambien los numeros no se da
>[!question] ¬øPorque los strings son inmutables
> De lo contrar√≠o, a python le tocar√≠a ver si en el espacio contiguo cobe lo que se esta buscando, entonces tiene que mover TODO a otra posici√≥n de memoria donde si quepa.

* Y las referencias repetidas tampoco representan gasto inecesario de memoria.
>[!example] En el caso de temp de un slicing
> Cuando se genera un slicing, referenciamos lo n√∫meros de prime, pero cuando cambiamos el temp, por ejemplo con ```temp[2]=15``` entonces el **puntero** cambia a apuntar a 15
> **Note que:** Entonces por eso es que una lista saca como objeto un string.


>[!example] ```data = [0]*8 ```
> Esto es un ejemplo de repetici√≥n de listas.
> En este escenario es seguro, pues el 0 no se puede cambiar. **ESTO NO SE DEBE HACER CUANDO SE REFERENCIA UN MUTABLE**
> Uno no hace esto a menos que lo haga con mutables.

* La pregunta ahora, es como hago una inicializaci√≥n segura.
	* ...con un ciclo for... que anticlim√°tico.
##### append y extend
* append agrega una posici√≥n
* Y extend crea referencias para cada uno de los datos de la otra lista
* Note que estas funciones en efecto me generan la necesidad de copiar toda la lista y volver a organizar.
#### Arreglos referenciales vs Arreglos compactos
* Los referenciales tienen un paso m√°s
	* Sin embargo siguen siendo $O[1]$
* La cuesti√≥n de referencia se basa en el espacio que ocupan cada uno, note que en los referenciales van a ocupar m√°s espacio en promedio.
### Complejidad temporal de los arreglos 
* Acceso en arreglos
* La inserci√≥n al inicio interno de arreglos es $O[n]$
	* As√≠ es la vida
* La eliminaci√≥n al inicio tmb va a ser $O[n]$
	* Los arreglos deben ser contiguos, nada que hacer.
* Inserci√≥n al final si es $O[1]$
* Las secuencias en python tienen un tama√±o fijo al crearse
	* `tuples` y `str` son inmutables
	* Pero las `list` si pueden cambiar de tama√±o
	* Python creo sin embargo un arreglo con m√°s espacio, normalmente el doble.
	* El problema es que cuando esto pasa, la inserci√≥n al final ya no es ~~$O[1]$~~, toca copiar y buscar en memoria, como copia cada cosa entonces la complejidad temporal ser√° $O[n]$, pero se llama `O[1] amortizado`.
	* ¬øPero la busqueda de espacio vacios en memor√≠a son O[1]?
		* M√°s o menos, pero se asume que si
* Eliminaci√≥n al final es $O[1]$
	* Implementa facil
	* Eliminar a√∫n m√°s f√°cil
* Toca tener cuidado con el tama√±o inicializado
	* Desplazar es costoso
![[Pasted image 20250819120354.png]]
### Lista como estructura recursiva
* La lista vac√≠a se entiende como el caso base
* Y luego 
- [ ] Ver documentaci√≥n de listas en y espec√≠ficamente de array list

### RAM [[Random Access Memories (10th Anniversary)]]
* Esa es la maravilla del random access memory, por eso tenemos las velocidades que tenemos

### Lista enlazada simple
* Single linked list
* Tiene que tener al menos una cabeza, que apunta hacia el siguiente dato, y el siguiente, hasta que se llega al final.
* De aqu√≠ sale el concepto de cola:
	* Que es la parte final del nodo final
* Note que cada nodo si guarda informaci√≥n
* El recorrido es navegar nodo a nodo desde la cabeza hasta la cola.
	* Y acabo en None.
* **Caracter√≠sticas:**
	* Operaciones comunes
	* Modificable
		*  Se pueden agregar nodos de forma eficiente
	* No indexada
		* No permite acceso directorio a un nodo por √≠ndice.
- Se basa en la colaboraci√≥n de m√∫ltiples objetos en memoria    
- Hay una instancia (objeto):
    - Representa la lista
        - Guarda referencia a la cabeza
- Opcional:
    - Guardar referencia a la cola:
        - Evita recorrer toda la lista
- Opcional:
    - Guardar contador de nodos
        - Evita recorrido total para calcular tama√±o
#### Complejidades
1.  Acceso `O(n)`
	1. No hay indices
	2. Tengo que pararme en la cabeza que es lo √∫nico que se conoce
2. Acceso al inicio `O(1)`
	1. Yo conozco la referencia inicial
	2. Esto es clave para escoger este tipo 
3. Eliminaci√≥n al inicio `O(1)`
4. Inserci√≥n/Eliminaci√≥n interna `O(n)`
5. Inserci√≥n a la cola es `O(n)` a menos que tenga referencia a la cola, en ese casi si es `O(1)`
6. Eliminaci√≥n en la cola es `O(n)` aunque tenga referencia. Pues tengo que devolverme uno y apuntarlo a `None`
## W04M1
![[Pasted image 20250825110838.png]]
* Array list ventajas
	* Implementaci√≥n sencilla
	* Almanecamiento contiguo
	* Eficiente par buscar por √≠ndice
	* Ananir o elimiar es O(1), excepto por la amortizaci√≥n, que no es O(n), pero si es m√°s que O(1)
* Array lists desventajas
	* Redimensionamiento y copiado es necesario
		* Hacer esto es O(n), pues implica copiar todos los elementos
	* Inserci√≥n y eliminaci√≥n es costoso
		* Desplaza elementos O(n)
* Single linked lists ventajas
	* Literalmente solo longitud flexible...
* Single linked lists desventajas
	* Literalmente todo lo dem√°s
* Ojo existe una diferencia entre head-tail lists y las listas enlazadas simples
	* La primera tiene obligatoriamente un puntero a la cola, ganando inserci√≥n al final O(1)
### Tipo de datos abstracto o TAD: Enfoque en **que**, no en el `como`
* TAD
	* Lo que nos permite interactuar entre operaciones
	* Una forma de abstracci√≥n, donde no importa la implementaci√≥n sino del serivcio
* Por ejemplo problema 1 necesita operacion1 y operacion2
* La pregunta es cual estructura va a ser m√°s eficiente para un TAD en es espec√≠fico
* Encapsula los datos y las operaciones permitidas,
	* Se aproxima a un api
* Encapsulamiento
	* Los datos solo se pueden manipular a trav√©s de las operaciones en el TAD, es b√°sicamente una api, entonces todo esta b√°sicamente bien definido desde el inicio
* La estructura de datos si se enfoca en el **c√≥mo**
	* Inserci√≥n en el inicio es necesaria, entonces es claro que se necesita un single linked list
![[Pasted image 20250825112034.png|center]]
#### La pregunta es cual elegir
* Depende del problema
* El tad me dice que requiero una lista
* La estructura de datos me dice que implementaci√≥n de list va a ser la mejor para el TAD en espec√≠fico.
* Note que el tad va a cambiar en funci√≥n del tiempo.
### Estructuras lineales 3
>[!note] Note que en la [documentaci√≥n del curso](https://isis1225devs.github.io/ISIS1225-Structure-Documentation/DataStructures.Graph.html) se realizo con la figura de diccionarios.
> Que pasa si lo hacemos con lista?
#### Primer intento de lista nativa
* Implementaremos `new_list()`, `is_fist()`, `add_list()`
* Primeras dos es `O(1)`
* Pero luego para hacer add_list() note que este a a tener complejidad O(n), pues la concatenaci√≥n nos retorna una nueva lista, por eso a python le toca copiar todo y por eso es O(n). luego si voy al insert, lo mismo, porque tengo que correr todo. Entonces grave porque la complejidad al inicio deber√≠a de O(1), no O(n).
#### Segundo intento, diccionario
* Implementaremos `new_list()`, `is_fist()`, `add_list()`
* Primeras dos es `O(1)` (Un poco m√°s complicado pero bien, es un diccionario con 3 llaves)
* is_empty() es O(1), solo verificamos el size
* finalmente el `add_first()`
	* Creo nuevo nodo, y lo apunto hacia la cabeza, y luego actualizo el puntero de la cabeza.

#### Ejemplo recorrido total de una lista enlazada simple
```run-python
def print_list_info(my_list):
    """
    Imprime la informaci√≥n de cada nodo en una lista simplemente enlazada.

    :param lst: Lista simplemente enlazada.
    :type lst: dict

    :return: Muestra la informaci√≥n en pantalla.
    :rtype: None
    
    >>> my_list = {"first": None, "last": None, "size": 0}
    >>> print_list_info(my_list)

    >>> my_list = {"first": {"info": 10, "next": {"info": 20, "next": {"info": 30, "next": None}}}, "size": 3}
    >>> print_list_info(my_list)
    10
    20
    30
    """
    if my_list["size"] > 0:
        current_node = my_list["first"]
        while current_node is not None:
            print(current_node["info"])
            current_node = current_node["next"]
```
## W04M2
### Lista enlazada doble
* En este escenario podemos devolvernos, ya no es unidireccionar, ahora tenemos apuntadores para el objeto previo, ya no solo para el next.
* Se usa un sentinela, pero no sentinela IP sino tener un header y trailer siempre iguales. 
* Next, prep, next, prep hasta trailer que esta marcado con el fin. directamente.
* Centinela
	* Nos permite simplificar los casos especiales
	* No almacena datos
	* Permanecen inmutables
	* Solo cambian los nodos inbtermedios
	* Son opcionales pero
	* Simplifican la l√≥gica de implementaci√≥n
		* En especial la inserci√≥n o borrado
			* Siempre ser√°n entre nodos existentes.
* Complejidad temporal de la lista enlazada doble
	* El acceso secuencial debe recorrerse nodo por nodo desde la cabeza
	* Sin √≠ndices directos
	* No hay acceso inmediato a alg√∫n nodo espec√≠fico.
	* Entonces es `O(n)`
#### Inserci√≥n al inicio `O(1)`
1. Agregamos un nuevo nodo
2. Luego actualizamos las referencias
	1. Del nodo nuevo hay que apuntarlo al que era el nuevo, a la cabeza
	2. Luego actualizar la vabeza
	3. Finalmente actualizar el viejo al que ahora es el primero.
3. Como son solo actualizaciones O(1)
#### Eliminaci√≥n al inicio `O(1)`
* Acceso inmediato al nodo centinela header
* Actualziar las referencias, del header al next, y el next apuntarlo al header
* Si se gquiere la idea es que el nodo eliminado quitar sus punteros, apuntando a None.
#### Inserci√≥n interna `O(n)` (O eliminaci√≥n interna)
1. Tenemos que iterar hasta el nodo que queremos
2. Crear el nodo
3. Actualizar las cuatro referencias, de los dos nodos de lado y lado
4. Esto es O(n), porque tengo que ir nodo por nodo hasta llegar al que necesitamos
* El problema es que tengo que navegar desde el header o tailer
#### Inserci√≥n/Eliminaci√≥n al final `O(1)`
* Tengo acceso inmediato al final gracias al `trailer`, por lo tanto es una operaci√≥n r√°pida.
* Igual para eliminaci√≥n
#### Beneficio con double vs single
* La eliminaci√≥n al final es O(1) en double vs single que es O(1), lo cual es genial.
### Lista circular
* Cada nodo apunta a otro nodo
	* Formando un ciclo cerrado
* No hay un nodo apuntando a `None`
* Pero sin embargo sigo teniendo un head y un tail, la diferencia es que no tengo `None` al final.
* El problema es saber cuando detenerse
#### Circular single linked list
1. Acceso `O(n)`
	1. No existe acceso directo
	2. Es neesario recorrer todos los nodos hasta encontrar el deseado
	3. Complejidad en el peor caso O(n)
2. Inserci√≥n/Eliminaci√≥n al inicio `O(1)`
	1. Si uno tiene un puntero en la cola
		1. Se enlaza en nuevo nodo con el primero
	2. Misma l√≥gica que anteriormente
3. Inserci√≥n interna `O(n)`
	1. Se puede jugar con cosas, pero igualmente va a dar `O(n)`
4. Eliminaci√≥n al final `O(n)`
	1. Este es un resultado general para los single linked
#### Circular single linked list
1. Eliminaci√≥n al final `O(1)`
	1. √önica ganancia comparado a circular single linked list
### Lista doblemente enlazada vs Lista circular doble
1. Navegaci√≥n continua
	1. En doble se debe reiniciar manualmente
	2. Circular doble permite recorrido continuo
		1. √ötil en buffers, planificaci√≥n, videojuegos
	3. Misma complejidad, diferente utilidad
		1. Eficiencia igual
		2. Navegaci√≥n c√≠clica sin reinicios
			1. Lista circular doble
### Pilas y colas
Estructura de datos de comportamiento LIFO, first to enter, last to leave.
* √öltimo en entrar, primer en salir (Last In, First Out)
* Principales operaciones
	* `push()`
		* A√±ade un elemento al tope de la pila
	* `pop()`
		* Me llevo una bandeja de arriba y me la llevo
	* `top()`
		* Me llevo la bandeja de arriba sin removerlo
### Implementaci√≥n Pila en python
```
# 1 S.push(e) = L.append(e)
#2 S.pop() = L.pop()
#3 S.top() = L[-1]
#4 S.is_empty() = len(L) == 0
#5 len(S) =  len(L)
```
```run-python
# El len funciona como queremos en python
print(len([0,1,2]))

```
1. En este  caso solamente agrego al final, entonecs `O(1)*` amortizado
2. Pop es una eliminaci√≥n al final, no desplaza, `O(1)*` (amortizado)
3. Top, es un acceso por indice, `O(1)`
4. is_empty, es `O(1)`, porque tiene contador
5. len es `O(1)`, yay.

* Es muy eficiente, salvo los casos amoritzados
### Cola
Comportamiento fijo, primero en entrar, primero en salir, es el opuesto de la pila. Es un ascensor b√°sicamente.
1. Enqueue
	1. A√±ade elemento al ==final==.
2. Dequeue
	1. Remueve y retorna elemento del ==frente==.
3. Peek
	1. Retorna elemento del final sin quitarlo. Similar al Dequeue, pero solo consulta.
* Algunos ejemplos son
	* Imprimir elementos 
	* Ascensor
	* Colar
### ~~Cola en python~~ Cola con single linked list
**No se deber√≠a usar**, pues todas las funciones van a ser `O(n)`
1. Enqueue
	1. A√±ade elemento al ==final==.
		1. Se puede implemetar con add_last, `O(1)`
2. Dequeue
	1. Remueve y retorna elemento del ==frente==.
		1. Se puede usar delete_last `O(1)`
3. Peek
	1. Retorna elemento del final sin quitarlo. Similar al Dequeue, pero solo consulta\
		1. remove_first es `O(1)`
## W05M1
### Repaso de recursi√≥n
* Esta se compone por el caso base y por el caso recursivo
* Estos componentes son los obligatorios 
* No todos los problemas se pueden hacer de forma, pero se tiene que dar que el problema se pueda dividir en varios subplroblemas.
* Referirse a [[ISIS1225#Recursividad]]
### Complejidad temporal
* Los casos base son por lo genera `O(1)`, pues son comparaciones y solo tienenun retorno, por lo general
* Luego en la parte recursiva vemos una complejidad temporal diferente, cada llamada genera una llamada adicional, hasta llegar a 1, en el caso de `factorial(n)` vemos que hay un total de `n` activaciones.
### Call Stack y complejidad espacial
* ALmacena invocaciones a funciones y variables locales
	* Durante la ejecuci√≥n de un programa
* Entonces cada vez que se llama una funci√≥n se llaman frames.
* El contexto se desapila solo cuando la funci√≥n termina.
	* As√≠ sabe `python` cual fibonachi va primero, b√°sicamente.
* 
## ISIS1225EVA1OP
### Algoritmos y Complejidad
- **Caracter√≠sticas**: precisos, correctos, determin√≠sticos, finitos.  
- **Cualidades**: generales, eficientes.  
- **An√°lisis**:
  - Temporal ‚Üí n√∫mero de operaciones.
  - Espacial ‚Üí memoria usada.
- **Notaciones**:
  - Œò: crecimiento exacto (te√≥rico, dif√≠cil).
  - Œ©: mejor caso (cota inferior).
  - O: peor caso (cota superior, m√°s usada).

---
### √ìrdenes de crecimiento

| Orden       | Descripci√≥n | Ejemplo |
|-------------|-------------|---------|
| **O(1)**    | Constante   | Acceso por √≠ndice |
| **O(log n)**| Logar√≠tmico | B√∫squeda binaria |
| **O(n)**    | Lineal      | Recorrer lista |
| **O(n log n)** | Lineal√≠tmico | Merge/Quick sort |
| **O(n¬≤)**   | Cuadr√°tico  | Doble bucle |
| **O(n¬≥)**   | C√∫bico      | Triple bucle |
| **O(2‚Åø)**   | Exponencial | Fibonacci recursivo |

**Ejemplo O(1):**
```run-python
def es_par(n): return n % 2 == 0
print(es_par(10))
````

**Ejemplo O(log n):**

```run-python
def busqueda_binaria(lista, x):
    i, j = 0, len(lista)-1
    while i <= j:
        m = (i+j)//2
        if lista[m] == x: return m
        elif lista[m] < x: i = m+1
        else: j = m-1
    return None
print(busqueda_binaria([1,2,3,4,5,6], 5))
```

---

### Complejidad Espacial

|Caso|Espacio total|Espacio auxiliar|
|---|---|---|
|Duplicar lista|O(n)|O(n)|
|Ordenar copia|O(n)|O(n)|
|Ordenar in-place|O(n)|O(1)|

**Ejemplo espacio O(n):**

```run-python
def duplicar_lista(lista): return [x*2 for x in lista]
print(duplicar_lista([1,2,3]))
```

---

### Recursividad

- **Caso base**: condici√≥n que detiene la recursi√≥n.
    
- **Caso recursivo**: llamada a problema m√°s peque√±o.
    
- **Reglas**: siempre converger al caso base, operaciones posteriores bien definidas.
    

**Factorial (O(n)):**

```run-python
def factorial(n): return 1 if n <= 1 else n * factorial(n-1)
print(factorial(5))
```

**Fibonacci (O(2‚Åø)):**

```run-python
def fibonacci(n): return n if n < 2 else fibonacci(n-1)+fibonacci(n-2)
print(fibonacci(6))
```

---

### Iteraci√≥n vs Recursi√≥n

|Aspecto|Iteraci√≥n|Recursi√≥n|
|---|---|---|
|Espacio|O(1)|O(n) por pila|
|Eficiencia|Mejor|Peor si no es tail|
|Ejemplo|ciclos for/while|factorial/fibonacci|

**Iterativo:**

```run-python
def factorial_iter(n):
    res = 1
    for i in range(2, n+1): res *= i
    return res
print(factorial_iter(5))
```

---

### Listas

|Tipo|Ventajas|Desventajas|Complejidad clave|
|---|---|---|---|
|**Array list**|Acceso por √≠ndice O(1), f√°cil de implementar|Inserci√≥n/elim. inicio O(n), redimensionamiento O(n)|Acceso O(1), inserci√≥n inicio O(n)|
|**Single linked list**|Inserci√≥n/elim. inicio O(1), tama√±o flexible|Acceso secuencial O(n)|Acceso O(n), insertar inicio O(1)|
|**Doble**|Navegaci√≥n adelante/atr√°s|M√°s punteros ‚Üí m√°s memoria|Inserci√≥n/elim. fin O(1)|
|**Circular**|Navegaci√≥n continua|Dif√≠cil detectar fin|Inserci√≥n inicio O(1)|

**Ejemplo Linked List:**

```run-python
class Node:
    def __init__(self, info, nxt=None):
        self.info, self.next = info, nxt
n3 = Node(30); n2 = Node(20, n3); n1 = Node(10, n2)
print(n1.info, n1.next.info, n1.next.next.info)
```

---

### Pilas (Stack) ‚Äì LIFO

- √öltimo en entrar, primero en salir.
    
- Operaciones:
    
    - `push(e)` ‚Üí a√±adir al tope.
        
    - `pop()` ‚Üí eliminar del tope.
        
    - `top()` ‚Üí consultar sin eliminar.
        
- Complejidad: todas **O(1)**.
    

**Ejemplo:**

```run-python
stack = []
stack.append(1)   # push
stack.append(2)
print(stack.pop()) # 2
print(stack[-1])  # top ‚Üí 1
```

---

### Colas (Queue) ‚Äì FIFO

- Primero en entrar, primero en salir.
    
- Operaciones:
    
    - `enqueue(e)` ‚Üí a√±adir al final.
        
    - `dequeue()` ‚Üí eliminar del frente.
        
    - `peek()` ‚Üí consultar frente.
        
- Implementaci√≥n eficiente con **linked list**: `enqueue` y `dequeue` O(1).
    

**Ejemplo:**

```run-python
from collections import deque
cola = deque()
cola.append(1); cola.append(2)   # enqueue
print(cola.popleft())  # 1
print(cola[0])         # peek ‚Üí 2
```
## W05M2
### Iteracion en python
* Un iterador es un obejto que representa un flujo de datos
* Se consume elemento por elemento sin llenar la memor√≠a
* Implementar el protocolo `__iter__` y `__next__`
	* Esta es la definici√≥n de u protocolo iterador
* Cada vez que se ejecuta un ciclo for se va a terminar ejecutando un `StopIteration`
```run-python
lista = [10,20,30]
Iterador = iter(lista)
print(next(Iterador))
print(next(Iterador))
print(next(Iterador)) # Aqu√≠ ya se consumio la lista 
print(next(Iterador))
print(next(Iterador))
print(next(Iterador))
```
* Y si ya es un iterable, retorna a si mismo.
* Y de hecho los `for` no es otra cosa que enmascarar el protocolo `iter()` y `next()`, cuando for llama un iterable, de hecho lo pasa por `iter()`, para aja, volverlo iterable.
	* En cada iteraci√≥n el for llama a `next()`
* Al final se lanza un `StopIteration` cuando la lista esta vac√≠a, esto no sale como tal, solo sale que el `for` termino por ejemplo.
```run-python
texto = "Hola"
iterador = iter(texto)  # Se llama manualmente a iter()
print(next(iterador))  # ‚Üí 'H'
print(next(iterador))  # ‚Üí 'o'
print(next(iterador))  # ‚Üí 'l'
print(next(iterador))  # ‚Üí 'a'
print(next(iterador))  # ‚Üí StopIteration

# Internamente se ve as√≠ 
for letra in "Hola":
	print(letra)
```
![[Pasted image 20250902111438.png|center]]

* Las razones para usar esto directamente es control total o para implementaci√≥n lazy.
### Implementaci√≥n de un doble lista enlazada
* Recordemos esta tiene doble enlazamiento `next` y `prev`
* Estos algoritmos son un poco m√°s forgiven porque en la ==practica de hecho son m√°s faciles==.
* Una pregunta del parcial va a ser as√≠.
```run-python
def new_double_node(element):
    """
    Crea un nodo para una lista doblemente enlazada.

    El nodo contiene:
    - info: Informaci√≥n almacenada en el nodo.
    - next: Referencia al siguiente nodo, inicializada en None.
    - prev: Referencia al nodo anterior, inicializada en None.

    :param element: Elemento del nodo.
    :type element: any

    :return: Nodo reci√©n creado.
    :rtype: dict que representa a un nodo en una lista doblemente enlazada.
    """
    return {"info": element, "next": None, "prev": None}
```
* Nuestra implementaci√≥n es identica exepto por el `prev`, que me permite navegar para adelante y para atr√°s.
```run-python
def new_double_list():
    """
    Crea una lista doblemente enlazada vac√≠a con nodos centinela.

    :return: Lista vac√≠a reci√©n creada.
    :rtype: dict que representa una lista doblemente enlazada.
    """
    header = {"info": None, "next": None, "prev": None}  # Nodo centinela inicial
    trailer = {"info": None, "next": None, "prev": header}  # Nodo centinela final
    header["next"] = trailer

    return {"header": header, "trailer": trailer, "size": 0}
```
* Note que esta lista tienen centinelas, esto la hace m√°s sencilla, en especial para la implementaci√≥n en el parcial 1.
* Muy bonito como se define
```run-python
def is_empty(my_list):
    """
    Verifica si la lista doblemente enlazada est√° vac√≠a.
    """
    return my_list["size"] == 0
def size(my_list):
    """
    Retorna el tama√±o de la lista doblemente enlazada.
    """
    return my_list["size"] # Note que el size NO TIENEN EN CUENTA ni header ni tail
```
### Inserci√≥n al inicio O(1)
```run-python
def add_first(my_list, element):
	new_node = new_double_node(element)
	new_node["next"] = my_list["header"]["next"]
	new_node["prev"] = my_list["header"]
	my_list["header"]["next"]["prev"] = new_node
	my_list["header"]["next"] = new_node
	my_list["size"] += 1
	return my_list
```

### Inserci√≥n al final `O(1)`

### Hablemos del parcial
* Hoja de bono de recursividad
* Primer punto del parcial se entrega una funci√≥n iterativa y hay que **pasarla a recursiva**.
	* Posiblemente pregunte complejidades **CT** y **CE**.
* Repasando lo de **CE** con espacio o memor√≠a adicional, esta NO INCLUYE LOS STACKS, porque desde que se entra a python, ==main== es lo primero que se apila, pues entonces TODO usar√≠a espacio adicional.
* En general en los algoritmos recursivos no hay espacio adicional.
* En el bono, para descomponer el bono, se deb√≠a descomponer el n√∫mero el problema es que al hacer 1000//10, esto no es un proceso lineal `O(n)`, sino es `O(log n)`, pues no estoy procesando `n` veces mi input sino `log n` veces.
* Un algoritmo iterativo clasico y lo transformamos a recursivo.
## W06M1
* Review de complejidad espacial, temporal, y ==estabilidad==.
	* Memoria extra es todo lo que se cree extra pero sin incluir el espacio de la lista original y tampoco incluye el call stack de python... pues esto implicar√≠a que todo crea espacio adicional.
* Los criterios de estabilidad tienen en cuenta 
	* Estable, hace que el arreglo ordenado mantenga el orden relativo de los elementos a medida que entraron a la lista original. 
	* En el caso inestable no se considera esto. El orden relativo cambia.
* Criterios de eficiencia espacial
	* In place
		* Realiza el ordenamiento modificando la estructura original
			* Minimiza el uso de memoria adicional
		* Ideal para restricciones de espacio.
	* Not in place
		* Requiere memoria adicional
		* Crea estructuras nuevas
		* Mayor consumo de espacio
			* hasta `O(n)`
### Selection sort (Super **obsoleto**)
- Busca el menor elemento en la lista
    - Intercambi√°ndolo con elemento en la posici√≥n cero
- Busca el 2do menor elemento en la lista
    - Intercambi√°ndolo con elemento en la posici√≥n 1
- Buscar el i-√©simo menor elemento en la lista
    - Intercambi√°ndolo con elemento en la posici√≥n (i-1)
- Es m√°s que un `O(n)`
- Su complejidad temporal es una serie aritm√©tica, que se vuelve ==`O(n^2)`== (Comparaciones)
- Si esta todo desordenado esta hace `O(n)` cambios (Swaping)
- la espacial es `O(1)`
- Y para tirarle sal a la herida es **inestable**
### Insertion sort
* Inserta donde corresponde
* Por repeticiones es `O(n^2)`
* En el peor caso los movimientos tambi√©n son `O(n^2)`
* Si la lista ya esta ordenada, entonces es `O(n)`, esto es mejor que el selection sort
* Adem√°s es in-place y es estable
### Shell sort
* Wtf
* Esto lo hizo Robert no se quien, muy pilo el hombre la verdad
* `O(n^1.5)`, pero esto es para secuencias probadas de forma emp√≠rica.
* Es ==inestable==, porque yo conf√≠o en el profe
	* Los saltos joden las posiciones
## W06M2
### Opciones recursivas
* Complejidad espacial en el caso recursiva se hace en el peor uso de la memoria
* Incluye variables, estructuras de datos y el tama√±o del stack
	* Ojo con el `espacio auxiliar`
### Quick Sort 
>[!note] Quick sort lo usa java

>[!note] Note que hay Quick Sort in place y no in place

* Divide seleccionando un pivote y organiza los elementos en tres conjuntos, menores iguales y mayores del pivote
* Luego Ordena los subconjuntos de elementos menores y mayores
* Finalmente junta los subconjuntos
* El pivote puede ser el 1, -1 o el de la mitad. 
	* Hay dobles pivotes tambi√©n
* Los elementos pueden estar en un posici√≥n no clara respecto al pivote
* El proceso de partici√≥n deja un pivote en su lugar.
1. Caso base
	1. Lista vac√≠a o un solo elemento
		1. No es necesario ordenarla
	2. low>= high
		1. Quiere decir que la sublista llega a tener 0 o 1 elementos
			1. Ya est√° ordenada
2. Casos recursivos
	1. Si low< high si se aplica el caso recursivo
	2. Se seleciona un pivote
	3. Se particiona en el pivote
		1. El pivote se pone en la mitad, en medio de los mayores y minores
		2. Luego se llama `quick_sort()` en las listas de los lados

```run-python
def quick_sort(arr, low = 0, high = None): # Aqu√≠ tenemos valores por defecto, esto no significa que no la podamos llamar con otro valor
	if high = None:
		high = len(arr)-1 
# quick_sort(arr) todo pasa con low=0 y el high se ajusta a len(arr)-1
# Luego con quick_sort(arr, 2, 5) ahora la funci√≥n solo ordena del indice 2 al 5
# high= None es una bandera para decirle hasta donde debe ordenar
	def partition(lst, low, high):
		pivot = lst[high] # toma el √∫ltimo en la lista
		i = low
		for j in range(low,high):
			if lst[j]<pivot:
				lst[i], lst[j] = lst[j], lst[i]
				i+=1
		lst[i], lst[high] = lst[high], lst[i] # Esto pone el pivote en la posici√≥n correta
		return i
	if low < high:
		pivot_index-partition(arr,low,high)
		quick_sort(arr,low,pivot_index -1)
		quick_sort(arr,pivot_index +1, high)
	
```

* Permite ordenar ciertas partes de la funci√≥n solamente
* Hace que la funci√≥n sea m√°s utilizable y autocontenida

>[!info] Sobre funciones anidadas o auxiliaries
>* Permiten separar partes de algoritmos que queremos implementar
>* Es una funci√≥n definida dentro de otra funci√≥n
>* Permite encapsular l√≥gica espec√≠fica para que solo la funci√≥n que la contiene la vea.

>[!info] Complejidad temporal
> `O(n log n)` Para el caso promedio
> Para el peor caso, va a ser `O(n^2)`
> Mejor caso `O(n)`

>[!info] Complejidad espacial
>`O(1)`, pues es implace, ==SEGURO?==
> Pero ojo, el stack va a irse llenando, aunque note que no se duplica la lista porque lo que hacemos
> Pero entonces por el stack va a ser `O(log n)` en el mejor caso y el caso promedio por la cantidad de llamadas que va a generar, y en este escenario el peor caso va a ser `O(n)`

>[!warning] Estabilidad
>* Note que como el pivote es random, no hay un mecanismo de protecci√≥n ante el orden relativo, entonces eso quiere decir que es ==inestable==. 

### Merge sort
* Se divide la secuencia en dos mitades aprox iguales
* Se divide hasta que cada sublista tenga un solo elemento
```run-python
mid = len(arr)//2
lh = merge_sort(arr[:mid])
rh = merge_sort(arr[mid:])
# Pero note que aqu√≠ estamos creando una nueva lista cada vez que la cortamos
```

* La complejidad temporal empieza por ser log n, pues siempre partimos por mitades
* Siempre se eta realizando entonces `O(n log n)`
* Ahora tiene problemas pues la complejidad espacial es `O(n)`, gracias a las estructuras extras
* Pero... ==ES ESTABLE==
* Aunque ==no es in-place== :c
## W07M1
* Pareja de llave-valor 
* Tabla de s√≠mbolos/Tabla de Hash o diccionarios
	* Son mecanismos abstractos que permiten almacenar grandes cantidades de informaci√≥n
	* La idea es que la complejidad siga tendiendo a `O(1)`.
* Las llaves deben ser √∫nicas
* Los valores si pueden estar repetidos
* Y a diferencia de los arreglos
	* Las llaves (√≠ndices) no deben ser consecutivas o siquiera ser num√©ricas.
	* Las llaves deben ser comparables e inmutables
		* Ej `int`, `float`, `str`, `bool`
		* Entonces las listas y dic no pueden ser llaves pues son mutables
			* Aunque si puede ser parte de la informaci√≥n
	* Las llaves no deber√≠an ser nulas.
* Las llaves deben ser `hasheables`
	* Objeto con un hash code consistente durante su vida √∫til
### Tabla de hash con direccionamiento directo
* Para parcial
* El problema es que va a ser `O(1)` pero amortizado de forma severa pues no podemos reservar tiempo al 100%
* Las llaves no siempre son enteros, las llaves normalmente son autoexplicativas
* El arreglo debe tener un tama√±o razonable
	* Seg√∫n la memoria
* Nosotros queremos conservar acceso O(1) a pesar de tener la llave m√°s compleja, pero entonces con la `hash function` de esta forma podemos simplificar el problema mapeando los nombres complejos a un espacio en espec√≠fico.
* Si el n√∫mero de llaves es mayor a la cantidad de espacio que tenemos siempre vamos a tener colisiones y no se puede devolver.
### Propiedades de la funci√≥n hash
* El proceso debe ser supremamente liviano para que sea `O(1)`.
* Deben ser una aritm√©tica super b√°sica para asegurar `O(1)`.
	* Eficiente en computo, porque de lo contrario todo va a ser `O(n)` o peor
* Y las debe distribuir uniformemente.
* Debe ser consistente, y de lo contrario va a ser ==completamente in√∫til==.
### REducci√≥n de colisiones
1. El nombre complejo se convierte en un entero, que puede ser negativo y que no tiene nada que ver con el nombre. Esto hace que sea totalmente diverso. (Funci√≥n de hash)
2. Luego viene la funci√≥n de compresi√≥n. Esta me entrega un indice.
	1. Esta funci√≥n debe saber el tama√±o del arreglo.
### `hash()` en Python
* Retorna el valor hash del objeto si lo tiene, es decir debe ser inmutable
* Puede variar entre ejecuciones y las versiones del interprete.
	* Es valida durante la ejecuci√≥n del interprete
	* No hay persistencia global
* Ahora, esto no es el indice en el array... para hacer esto vamos a aplicar la funci√≥n de compresi√≥n
### Funciones de compresi√≥n
1. M√©todo de la divisi√≥n
	1. Entonces en este caso debemos usar un N primo, para que no sea divisor de ning√∫n numero dentro del hash.
2. $|x|mod(N)$ 
	1. El mod no me deja pasar del l√≠mite, pues no se pasa del rango del arreglo N.
	2. Si tenemos N=10 y sacamos x%N, estar√° en el rango 0 a 9. Super trivial, ¬°pero funciona!

==Promesa: O1 para inserciones, eliminaciones y acceso==
## W07M2
* "Eduardo" --> Had code, que es un int, + o - y es grande o largo
* Umbral de rehashing, es precisamente el factor de carga a partir del cual se considera necesario aumentar o disminuir la reserva de llaves en un diccionario (hash table).
## W08M1 Hoja de trabajo
## W08M2
### Separate chaining
- [x] #academy/202520/ISIS1225 Hoja de trabajo de separate chaining üìÖ 2025-10-06 ‚úÖ 2025-10-06
* Aparece una nueva estructura, que es un arreglo de listas, donde cada indice da acceso a un bucket (lista)
* En cada posici√≥n se apunta hacia un alista, una estructura **2D**. Cada bucket almacena 0, 1 o 2 elementos.
* La idea es que los buckets no est√©n densamente poblanos, as√≠ en t√©rminos de complejidad temporal no se torna `O(n)`
* EL escenario realista es que hay que evitar b√∫squedas secuenciales muy extensas
## W10M1

<iframe src="https://eerosales24.github.io/eda_2025_20/#/" width="100%" height="500px"></iframe>
### BST o Binary Search Tree 
* Tienen unas reglas que permiten optimizar las b√∫squedas de muy buena forma
* Y cuando esta balanceado entonces su altura es igual a `log2(n)` comparaciones.
	* Esto ojo **si esta balanceado**
* Un √°rbol balanceado mantiene su altura lo m√°s baja posible, lo m√°s alineada a la izquierda posible.
* Y la complejidad se va a mantener `O(log n )`, pues siempre estamos descartando del 50 al 50, luego por esto es logar√≠tmico.
	* Esto no es un mapa, las llaves est√°n ordenadas.
* El problema es que si el √°rbol es ==degenerado== entones en este escenario la complejidad va a ser `O(n)` en el peor caso.
* Ahora sale un caso bien interesante donde el arbol degenerado surge de obtener datos ordenados, well shit. Entonces hay que hacer casos muy espec√≠ficos, esto es largo y complejo (150+ diapositivas).
### √Årbol binario 
* El an√°lisis de la ra√≠z no basta para decir que el arbol esta equilibrado, tenemos que ir bajando por cada Linea hasta terminar de efectivamente decir que esta equilibrado
* El truco es que el test no es un si solo si. Y se debe continuar ==AS√ç!== con cada nodo como  una ra√≠z
* ![[Pasted image 20251014114206.png|500]] 
* EL factor de equilibrio de calcula como
```
  ‚àí1 ‚â§ factor de equilibrio ‚â§ 1 para todo nodo factor de equilibrio = altura del sub√°rbol derecho ‚àí altura del sub√°rbol izquierdo
  ```

>[!info] √Årbol binario
>* Tipo ed √°rbol donde cada nodo 
>* Puede tener como m√°ximo 2 hijos
>	* Uno a la izquierda 
>	* Uno a la derecha
>* Esto quiere decir que el **orden = 2**, pero el grado no es necesariamente 2.
>* El √°rbol que puede tener m√°s de dos es el ==√°rbol n-ario==
>* Hay un distinci√≥n entre √°rbol perfecto y √°rbol no perfecto
>	* En uno todas las hojas est√°n en el mimo tivel
>* El √°rbol binario completo
>	* Todos los niveles excepto el √∫ltimo esta lleno, pero este tiene todos lo nodos a la izquierda como le sea posible.

>[!info] Definici√≥n recursiva de un arbol
> * Esta el caso base
> 	* Un √°rbol vac√≠o
> * Esta el caso recursivo
> 	* Cada nodo es la ra√≠z de un sub√°rbol
> 	* Cada sub√°rbol en un √°rbol

* EL **sub-arbol**  es el arbol generado de una secci√≥n determinada del √°rbol.
* El **grado** es el n√∫mero de hijos que tiene un nodo (El m√°ximo)
	* El grado me lo da el nodo m√°ximo
* El **peso** es la cantidad total de nodos, este se puede pre-calcular dependiendo el grado del arbol
	* Usualmente se busca minimizar el peso del arbol.
* Luego esta el n√∫mero de **orden**, que es el n√∫mero m√°ximo de hijos que puede tener un nodo
![[Pasted image 20251014112420.png]]
* La **altura** es el n√∫mero m√°ximo de niveles de un √°rbol
![[Pasted image 20251014112344.png]]
* **Nivel** es igual a profundad
* Tambi√©n esta el concepto de nivel, que es cada generac√≠on del arbol
	* El arbol vac√≠o no tiene 0 niveles
	* La ra√≠z siempre est√° en el nivel 0
	* Los hijos de la raiz estan en nivel 1
	* Los nietos de la ra√≠z est√°n en el nivel 2
		* etc, etc, etc
* Un √°rbol vac√≠o es aquel que no tiene ning√∫n nodo no arista, pero sigue siendo un √°rbol
	* ¬øTiene un nodo ra√≠z? como se que esta definido entonces
* Tenemos tres partes importantes, la ==ra√≠z==, **rama** y __hoja__
	* La ra√≠z no tiene padrea
	* La rama es un nodo que no es ni la ra√≠z y tiene al menos un hijo
	* Las hojas son nodos sin hijos, aka el final de una rama
	* Siempre tenemos una relaci√≥n de padres/hijos
		* Tambi√©n esta el concepto de hermano que es el que comparte un mismo padre en com√∫n dentro de la estructura.
* Empezamos con arboles binarios, un precursor de los grafos
* Los mapas nos mantienen a todo en un mismo nivel
* Un arbol es una estructura de datos jer√°rquica que NO ES LINEAL. A diferencia de los arreglos. Donde hay elemento superiores e inferiores.
* Los caminos son √∫nicos dentro del arbol y nos dirigen entre los nodos pasando por las aristas.
## W11M1
Ordenamiento de arboles BST
* Recorridos en post-orden
	* La ra√≠z queda de √∫ltima
* In-orden
* Pre-orden
Idea de separaci√≥n de responsabilidades
* La idea es separar una funci√≥n principal y una funci√≥n recursiva, de tal forma que se implemente de forma limpia
## EVA3OP
### √Årboles binarios
* Mapas ordenados (Tabla de simbolos)
* Con elementos comparables
* Permiten acceder a subsets de la informaci√≥n
* Los √°rboles nos permiten hacer esto reduciendo los costes de eficiencia.
* Un √°rbol es una estructura jer√°rquica no lineal
	* Est√°n compuestos por nodos y aristas, los nodos son aquellos que guardan la informaci√≥n en forma de pareja llave-valor
* Estos tienen ra√≠ces, ramas y hojas.
>[!example] ¬øQu√© es un √°rbol
>- **Nodo**: Unidad b√°sica de un √°rbol, contiene un dato
>- **Arista o enlace**: Conexi√≥n entre dos nodos
>- **Ra√≠z**: Nodo sin padre, est√° en la cima del √°rbol
>- **Hijo**: Nodo conectado y descendiente de otro nodo
>- **Padre**: Nodo que tiene al menos un hijo
>- **Hermano:** Nodo que comparte a un mismo padre en com√∫n dentro de la estructura
>- **Rama**: Nodo que no es la ra√≠z ¬†y que ademas tiene al menos un hijo
>	- Tambi√©n se puede llamar as√≠ a un camino completo
>- **Hoja**: Nodo sin hijos. Final de una rama


>[!example] ¬øQu√© es un √°rbol vac√≠o?
> √°rbol sin ning√∫n nodo o arista, obviamente.
> Es la m√≠nima expresi√≥n posible de √°rbol.
#### Niveles (Equivalente a profundidad) y otra terminolog√≠a
- Un √°rbol vac√≠o tiene 0 niveles
- La ra√≠z siempre est√° en el nivel 0
- Los hijos de la ra√≠z est√°n en el nivel 1
- Los "nietos" de la ra√≠z est√°n en el nivel 2
![[Pasted image 20251104065028.png]]
* Peso = N√∫mero total de nodos
* Grado = N√∫mero m√°ximo de hijos que tiene uno de los nodos del √°rbol.

#### √Årbol n-ario
* Cada nodo puede tener m√°ximo n hijos.
#### √Årbol binario lleno
![[Pasted image 20251104065336.png]]
#### √°rbol binario perfecto
![[Pasted image 20251104065409.png]]
#### √°rbol binario completo
![[Pasted image 20251104065512.png]]
#### √°rbol binario balanceado
* Un √°rbol binario es balanceado si cumple:
```
‚àí1 ‚â§ factor de equilibrio ‚â§ 1 para todo nodo
```
- Donde el **factor de equilibrio** se calcula como:
```
altura del sub√°rbol derecho ‚àí altura del sub√°rbol izquierdo
```
### BST I
* La complejidad temporal de buscar un n√∫mero en un √°rbol no ordenado va a tender a `O(n)` cuando este no esta ordenado. Entonces para hacer esta b√∫squeda efectiva tenemos que usar los √Årboles binarios de b√∫squeda:
>[!example] √Årbol binario de b√∫squeda
> - Cada nodo tiene una **llave √∫nica y comparable**
> - Cada nodo puede tener un **valor asociado**
> - Todos los nodos del **sub√°rbol izquierdo son menores**
> - Todos los nodos del **sub√°rbol derecho son mayores**
> - Cada sub√°rbol tambi√©n es un BST
* Note que el n√∫mero de comparaciones para encontrar un n√∫mero va a ser aproximadamente `h+1`.
#### Balance del BST
* Este esta balanceado cuando la altura sea proporcional a `log2(n).
	* Un √°rbol balanceado mantiene su altura m√°s baja
	* Esto garantiza operaciones eficientes
		* B√∫squeda
		* Inserci√≥n
		* Eliminaci√≥n
	* Siempre mantiene la complejidad a `O(log n)`
* Si el √°rbol no esta balanceado se asemeja a una estructura lineal y por lo tanto cosas como insertion se vuelven `O(n)`
#### Complejidad temporal de la b√∫squeda

- Mejor caso - **O(1)**

    - Si el elemento est√° en la ra√≠z
- Promedio - **O(log‚Å°n)**

    - √Årbol balanceado
- Peor caso - **O(n)**

    - √Årbol desbalanceado
#### Complejidad temporal de la inserci√≥n

- Mejor caso - **O(1)**

    - Si el elemento est√° en la ra√≠z
- Promedio - **O(log‚Å°n)**

    - √Årbol balanceado
- Peor caso - **O(n)**

    - √Årbol desbalanceado
#### Complejidad temporal de la eliminaci√≥n

- Mejor caso - **O(1)**

    - Si el elemento est√° en la ra√≠z
- Promedio - **O(log‚Å°n)**

    - √Årbol balanceado
- Peor caso - **O(n)**

    - √Årbol desbalanceado
### BST II - Recorridos en √°rboles BST
#### In-orden
* Permite acceder en orden ascendente en un BST
* Primero visita el subarbol izquierdo
	* Luego el nodo actual
	* Luego el nodo derecho
#### Pre-orden
* √ötil para copiar el √°rbol y obtener una expresi√≥n con parentes√≠s
* Primero se visita nodo actual
	* Luego sub√°rbol izquierdo
	* Luego sub√°rbol derecho
#### Post-orden
* √ötil para eliminado liberar nodos de un √°rbol (desde las hojas hasta la ra√≠z)
	* El √∫ltimo elemento visitado es la ra√≠z
* Primero se visita el subarbol izquierdo
	* Luego el subarbol derecho
	* Luego el nodo actual
### BST III - Implementaci√≥n
* implementaciones de `floor()` y `ceiling()`
### Red-black Tree RBT
* Se estudia el Left-leaning RBT
* Que es una variante del BST
* Que se equilibra despu√©s de inserciones y eliminaciones para evitar la degradaci√≥n del rendimiento
* Ideal para sistemas de bases de datos
* Es una estructura que permite acceso r√°pido
#### Propiedades del LLRBT

1. Al momento de insertar un nodo, este tiene un **enlace rojo**

    - Excepto **la ra√≠z que siempre es negra**
2. **Todo enlace rojo se inclina a la izquierda**

3. **No se permiten enlaces rojos a la derecha**

4. **No se permiten dos enlaces rojos consecutivos**
#### Operaciones de reestructuraci√≥n del √°rbol (1/3)

- Son las operaciones que el √°rbol aplica al **insertar** o **eliminar** nodos

    - Para mantener sus propiedades



- **Rotaci√≥n Izquierda**:

    - Endereza un enlace rojo derecho
        - Pas√°ndolo a la izquierda
            - Y subiendo el nodo que estaba a la derecha
- **Rotaci√≥n Derecha**:

    - Rompe una cadena de dos nodos rojos en la izquierda
        - Pasando el rojo hacia abajo
            - Y subiendo al hijo izquierdo
- **Cambio de color**:

    - Intercambia los colores del nodo padre y de sus dos hijos
        - Objetivo: Cantidad constante de nodos negros en cada camino
- **Validaci√≥n:**

    - Se verifica que el √°rbol cumpla con todas las propiedades
        - Si no es as√≠, se hacen las reestructuraciones necesarias
            - De manera recursiva hasta que se cumplan
![[Pasted image 20251104080154.png]]

## Simulacro EVA3
1. se debe implementar  `equals(root1, root2)`
	1. las soluciones en general van a tener que ser recursivas
	2. la idea es identificar si efectivamente dos arboles son iguales, la implementaci√≥n es muy benevolente eso si

```run-python
def equals(root1, root2):
"""
Retorna True si los √°rboles dados son id√©nticos estructuralmente y en contenido
"""
# Caso 1 Si ambos √°rboles est√°n vacios, se retorna True
	if root1.is_empty() and root2.is_empty():
		return True
# Caso 2 si un √°rbol es va√ßio y el otro no, la funci√≥n debe retornar False
	if root1.is_empty() != root2.is_empty():
		return False
# Caso 3 √°rboles con un solo id√©ntico la funci√≥n debe retornar True
	return root1.size_tree() == 1 and root1.size_tree() == 1 and root1["value"] == root1["value"]
# Caso 4 para √°rboles con un solo nodo diferente (misma llave, pero distinto valor o viceversa, la funci√≥n debe retornar False)
	# creo que ya se tiene en cuenta en el caso 3
```

2. Para este punto se pide determinar si un √°rbol es un BST perfecto, esto quiere decir que esta lleno y que todas las hojas est√©n al mismo nivel.
	1. Para esto se considera una funci√≥n que confirma si un arbol es √°rbol binario:
	2.
```run-python
def is_binary_tree(root):
	if root is None
		return True
	for k in root:
		if isinstance(root[k], dict) and k not in ("left", "right"):
			return False
	return is_binary_tree(root.get("left")) and is_binary_tree(root.get("right"))
```

luego entonces para hacer `is_perfect_bst(root)`:
```run-python
def is_perfect_bst(root):
"""
Para que un BST sea perfecto se debe cumplir que
- Sea BST
- Sea binario
- Es lleno (Es decir que todos los nodos esten llenos menos las hojas) osea todos los nodos tienen 0 o 2 hijos
"""
	return (
	is_binary_tree(root)
	and is_bst(root)
	and is_full_tree(root)
	and are_leaves_at_same_depth(root)
	)
```

luego por supuesto la idea entonces es implementar `is_bst(root)`, `is_full_tree(root)` y `are_leaves_at_same_depth(root)` (y t√©cnicamente toca implementar `is_binary_tree(root)` pero esa ya nos la dan porque son muy buenos)


---
# Recap

# Sources
1. https://eerosales24.github.io/eda_2025_20/#/
2. [ISIS1225](https://uniandes-my.sharepoint.com/:f:/g/personal/la_rodrigueza12_uniandes_edu_co/ElJPib3aVVZMo0CEiRYTsb4B9-y4gXH75X_CSaTBX-rKOA?e=wX3IgM)
___